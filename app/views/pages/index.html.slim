/ - cache ['form_result', @search], expires_in: 1.hour do
- cache ['form_result', @search], expires_in: 0.hour do
  - @anecdotes = Anecdotes::SearchService.call(@search)
  = form_with url: { action: 'index' },
          data: { turbolinks: false, turbo: false },
          method: :get,
          local: true,
          html: { class: 'm-4', 'data-turbo': false } do |form|
    .row.g-3.align-items-center
      .col
        = form.text_field :search,
                value: @search,
                class: 'form-control',
                placeholder: "Введите что-ть для поиска анекдота",
                autofocus: true
      .col-auto
        = form.submit "Искать", class: "btn btn-primary"

  - if @anecdotes.present?
    H2 Нашлось
    ul
      - @anecdotes.each do |anecdote|
        li
          = raw html(anecdote)
          span.text-secondary
            |  /
            = anecdote['score']
            |  /
            = anecdote['id']
H2 Примеры
.row.g-4.md-4
  = render partial: 'card', locals: { \
      title: 'Про пса и саппорт', \
      text: 'конкретно слов запроса нет в тексте найденного анекдота' }
  = render partial: 'card', locals: { \
      title: 'Не забывай', \
      text: 'ищем про "вспомнить", а находит и про "не забыть"' }
  = render partial: 'card', locals: { \
      title: 'как устроены вычислительные машины', \
      text: 'неожиданно' }
  = render partial: 'card', locals: { \
      title: 'About a Dog and Support', \
      text: 'вопрос на английском, ответ по-русски' }
  = render partial: 'card', locals: { \
      title: 'перекрасно', \
      text: 'описки (перекрасно -> прекрасно) не проблема' }
  = render partial: 'card', locals: { \
      title: 'кот и демократия', \
      text: 'демократия и выборы это точно не синонимы' }
  = render partial: 'card', locals: { \
      title: 'Postgres', \
      text: 'чисто Андрея Булыгина порадовать' }
  = render partial: 'card', locals: { \
      title: 'Альфа-тестер', \
      text: 'qa в нашей жизни' }

H2 Как это работает
ol
  li по каждому анекдоту делается OpenAi embedding (это 1536-размерный вектор чисел). Через их Api
  li сохраняется в Pinecone векторную БД
  li запрос на поиск тоже превращается в embedding
  li делается запрос в Pinecone на поиск ближайших к вопросу векторов
